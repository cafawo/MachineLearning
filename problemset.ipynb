{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Problem Set\n",
    "\n",
    "## Overview\n",
    "\n",
    "The focus of this problem set is on analyzing textual data from scientific papers. The primary aim is to develop an analysis pipeline that reads, pre-processes, analyzes, and displays textual data.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "### Data Sourcing\n",
    "\n",
    "The first stage involves sourcing and reading the textual content of scientific papers. You can find a few example PDF files in the `data/` directory. Please download additional papers of your choice — ensuring you analyze a total of at least 6 papers. The papers should relate to at least 2 different fields of your choosing; this could be within economics, but may also be as broad as medicine and physics. The choice is yours.\n",
    "\n",
    "Use an appropriate PDF reading library or tool to programmatically extract the text. An example is provided below, but you are free to use any Python library you prefer.\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "Pre-processing is a critical step aimed at cleaning and preparing the text data for analysis. In this problem set, we want to focus on sentences, so you should first tokenize the text into sentences. Further steps include:\n",
    "\n",
    "- Removing punctuation, numbers, and special characters (e.g. by using regular expressions).\n",
    "- Converting all text to a uniform case (usually lowercase), so the analysis is not case-sensitive.\n",
    "- Stop word removal, i.e., eliminating commonly used words (e.g., \"and\", \"the\", \"is\") that do not contribute significantly to the overall meaning and can skew the analysis.\n",
    "- Other potential pre-processing steps might include stemming and lemmatization, depending on the specific requirements and goals of the analysis (optional).\n",
    "\n",
    "### Analysis\n",
    "\n",
    "The final stage is the analysis of the pre-processed text to extract meaningful context. In this problem set, you have to create a t-SNE visualization of the tokens (sentences) that have already been pre-processed. Make sure to label each sentence based on the paper (or field) of its origin. In the t-SNE plot, you should color-code the sentences by paper or field. Ultimately, you should be able to visually infer how similar these papers are in a linguistic sense.\n",
    "\n",
    "You may pick any method you like (including the use of ChatGPT or OpenAI's API), as long as your approach is clearly documented.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Please note that the focus of this case is primarily on the execution of the tasks, **not** on the final results. The methods chosen are therefore of secondary importance and left to your discretion. However, your results must be **reproducible** with the submitted code. Emphasize clean coding, thorough commenting, and the appropriate use of Git/GitHub. Follow the guidelines laid out in [PEP 8 – Style Guide for Python Code](https://peps.python.org/pep-0008/).\n",
    "\n",
    "Your solutions should be contained in the Jupyter notebook `problemset.ipynb`, while data should be stored in the `data/` folder. **Everything** required to reproduce your results must be committed to your GitHub Classroom repository. For more details, see the \"Problem set\" section at: [wbk.ing/MachineLearning/](https://wbk.ing/MachineLearning/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting text from PDF files\n",
    "\n",
    "**(This is a suggestion, remove the code if you do not need it ...)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install pdfminer.six if you haven't already\n",
    "# You can install it using conda or pip, see \n",
    "  # https://anaconda.org/conda-forge/pdfminer.six\n",
    "  # https://pypi.org/project/pdfminer.six/\n",
    "\n",
    "# This will automatically install pdfminer via pip\n",
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import the required modules\n",
    "import os\n",
    "import re\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bajari-MachineLearningMethods-2015.pdf', 'cesifo1_wp6504.pdf', 'SSRN-id3567724.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Files in the data folder\n",
    "pdf_files = os.listdir(path='data')\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your PDF file\n",
    "pdf_file_path = os.path.join('data', pdf_files[2])\n",
    "\n",
    "# Extract text\n",
    "extracted_text = extract_text(pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Let me get back to you” –\n",
      "A machine learning approach to measuring\n",
      "non-answers∗\n",
      "\n",
      "Andreas Barth†\n",
      "\n",
      "Sasan Mansouri‡\n",
      "\n",
      "Fabian Woebbeking§\n",
      "\n",
      "April 1, 2022\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is the beginning of the extracted text\n",
    "print(extracted_text[0:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Let me get back to you    A machine learning approach to measuring non answers   Andreas Barth   Sasan Mansouri   Fabian Woebbeking   April          \n"
     ]
    }
   ],
   "source": [
    "# Example: Regex to remove all non-alphabetical characters and replace them with a space\n",
    "processed_text = re.sub('[^a-zA-Z]', ' ', extracted_text)\n",
    "\n",
    "print(processed_text[0:150])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "title": "Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
